{"cells":[{"cell_type":"markdown","metadata":{},"source":["<h1 style=\"font-family:verdana; color:crimson;\"> <center> Sentiment Analysis with NLP Tutorial </center> </h1>\n","<center><img src=\"https://hsto.org/webt/t6/sr/jr/t6srjrmjjmm6qn8gpld9emy4txu.gif\" width=\"500\"/></center>\n","\n","# AIM OF NOTEBOOK\n","Nowadays, user reviews are one of the most effective methods to buy a product, or not. These reviews can both give an idea to customers and be feedback for manufacturers since the reviews show the positive sides and the sides to be improved. In this project, sentiment analysis will be implemented in a product on Amazon, and each step of NLP and sentiment analysis will be explained. Summarily, this is a tutorial notebook that includes the implementation of NLP and Sentiment Analysis with a dataset that includes Amazon reviews.\n","\n","<div class=\"inner_cell\">\n","<div class=\"text_cell_render border-box-sizing rendered_html\">\n","<p></p><div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n","  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\" style = \"border:2px solid #581845;background-color:#581845; color:white; font-family:Verdana;text-align: center; font-size:140%;font-weight: Bold;\">Notebook Content</h3>\n","  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\" target=\"_self\" style = \"color:#581845; font-family:Verdana;text-align: center; font-size:130%;font-weight: Bold;\">Import Libraries and Check Data<span class=\"badge badge-primary badge-pill\">1</span></a>\n","  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\" aria-controls=\"messages\" target=\"_self\" style = \"color:#581845; font-family:Verdana;text-align: center; font-size:130%;font-weight: Bold;\">Text Preprocessing<span class=\"badge badge-primary badge-pill\">2</span></a>\n","  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"messages\" target=\"_self\" style = \"color:#581845; font-family:Verdana;text-align: center; font-size:130%;font-weight: Bold;\">Text Visualization<span class=\"badge badge-primary badge-pill\">3</span></a>\n","  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"messages\" target=\"_self\" style = \"color:#581845; font-family:Verdana;text-align: center; font-size:130%;font-weight: Bold;\">Sentiment Analysis<span class=\"badge badge-primary badge-pill\">4</span></a>\n"," <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#5\" role=\"tab\" aria-controls=\"messages\" target=\"_self\" style = \"color:#581845; font-family:Verdana;text-align: center; font-size:130%;font-weight: Bold;\">Feature Engineering<span class=\"badge badge-primary badge-pill\">5</span></a>\n","  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#6\" role=\"tab\" aria-controls=\"messages\" target=\"_self\" style = \"color:#581845; font-family:Verdana;text-align: center; font-size:130%;font-weight: Bold;\">Sentiment Modeling<span class=\"badge badge-primary badge-pill\">6</span></a>\n"," \n","    \n","</div>\n","</div>\n","</div>\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["<a id = \"1\"></a>\n","# 1. Import Libraries and Check Data üßê"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:23.971629Z","iopub.status.busy":"2022-11-01T23:05:23.970276Z","iopub.status.idle":"2022-11-01T23:05:23.99199Z","shell.execute_reply":"2022-11-01T23:05:23.990854Z","shell.execute_reply.started":"2022-11-01T23:05:23.971454Z"},"trusted":true},"outputs":[],"source":["# Download the libraries if you don't have\n","# !pip install nltk\n","# !pip install textblob\n","# !pip install wordcloud"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-11-01T23:05:28.683429Z","iopub.status.busy":"2022-11-01T23:05:28.683037Z","iopub.status.idle":"2022-11-01T23:05:30.594589Z","shell.execute_reply":"2022-11-01T23:05:30.59336Z","shell.execute_reply.started":"2022-11-01T23:05:28.683399Z"},"trusted":true},"outputs":[],"source":["# Basic Libraries üìö\n","# --------------------------------------\n","import numpy as np\n","import pandas as pd\n","\n","\n","# Plot library üìä\n","# --------------------------------------\n","import matplotlib.pyplot as plt\n","import plotly.graph_objs as go\n","import plotly.io as pio\n","\n","\n","#¬†NLP\n","# --------------------------------------\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.sentiment import SentimentIntensityAnalyzer\n","from textblob import Word, TextBlob\n","from wordcloud import WordCloud  #¬†visualization of words\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","\n","# Metrics üìê\n","# --------------------------------------\n","from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate\n","from sklearn.preprocessing import LabelEncoder\n","\n","\n","# Machine Learning Models ü§ñ\n","# --------------------------------------\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","\n","\n","# Customize to Remove Warnings and Better Observation üîß\n","# --------------------------------------------------------\n","from warnings import filterwarnings\n","filterwarnings('ignore')\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', 200)\n","pd.set_option('display.float_format', lambda x: '%.2f' % x)"]},{"cell_type":"markdown","metadata":{},"source":["<strong><h2>Bussines Problem</h2><p>\n","It aims to increase its sales by analyzing the comments received on Kozmos products, which makes production focused on home textiles and daily wear, and by improving their features according to the complaints received. In line with this goal, the comments will be labeled by performing sentiment analysis and a classification model will be created with the labeled data.</p>\n","    \n","    \n","<strong><h2>Dataset Story</h2></strong>\n","<blockquote><p><strong> The data set consists of the comments made for a certain product group, the title of the comment, the number of stars and the variables that indicate how many people found the comment useful. </strong></p>\n","</blockquote>\n","    \n","<br>\n","\n","\n","<li><strong>Total Features : 4</strong></li>\n","<li><strong>Total Row : 5611</strong> </li>\n","<li><strong>CSV File Size : 489 KB</strong></li>\n","\n","\n"," Sr. | Feature  | Description |\n","--- | --- | --- \n","1 | Star | Number of stars given to the product\n","2 | Helpful | Number of people who found the comment helpful\n","3 | Title | Title given to comment content, short comment\n","4 | Review | Review of the product\n","\n","---\n","   "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:30.596955Z","iopub.status.busy":"2022-11-01T23:05:30.59657Z","iopub.status.idle":"2022-11-01T23:05:31.563814Z","shell.execute_reply":"2022-11-01T23:05:31.56266Z","shell.execute_reply.started":"2022-11-01T23:05:30.596923Z"},"trusted":true},"outputs":[],"source":["df = pd.read_excel(\"../input/amazon-reviews/amazon.xlsx\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:31.566273Z","iopub.status.busy":"2022-11-01T23:05:31.565779Z","iopub.status.idle":"2022-11-01T23:05:31.577154Z","shell.execute_reply":"2022-11-01T23:05:31.575884Z","shell.execute_reply.started":"2022-11-01T23:05:31.566229Z"},"trusted":true},"outputs":[],"source":["df.isnull().sum()"]},{"cell_type":"markdown","metadata":{},"source":["Just drop the missing values without deep analysis for process."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:31.580358Z","iopub.status.busy":"2022-11-01T23:05:31.57992Z","iopub.status.idle":"2022-11-01T23:05:31.598527Z","shell.execute_reply":"2022-11-01T23:05:31.597577Z","shell.execute_reply.started":"2022-11-01T23:05:31.580325Z"},"trusted":true},"outputs":[],"source":["df.dropna(subset=['Review'], inplace=True)\n","df.isnull().sum()"]},{"cell_type":"markdown","metadata":{},"source":["<a id = \"2\"></a>\n","# 2. Text Preprocessing üìù\n","\n","<span class=\"label label-default\" style=\"background-color:#581845; border-radius:3px; font-weight: bold; font-family:Verdana; font-size:16px; color:#FBFAFC; \">Normalizing Case Folding</span>\n","\n","In the data set, some letters are uppercase and some letters are lowercase. The disadvantage of mixed lowercase-uppercase letters is that the same word may start with a capital letter at the beginning of the sentence while it is lowercase in the middle of the sentence. When we wanted to measure by creating word vectors and representing the word, we would not be able to distinguish words that are normally the same word because of uppercase and lowercase letters, and a measurement problem would arise. So, the frequency cannot be enriched."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:31.60042Z","iopub.status.busy":"2022-11-01T23:05:31.599625Z","iopub.status.idle":"2022-11-01T23:05:31.61546Z","shell.execute_reply":"2022-11-01T23:05:31.614429Z","shell.execute_reply.started":"2022-11-01T23:05:31.600386Z"},"trusted":true},"outputs":[],"source":["# to put all the characters in a standard\n","df['Review'] = df['Review'].str.lower()  #¬†make all lowercase"]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{},"source":["<span class=\"label label-default\" style=\"background-color:#581845; border-radius:3px; font-weight: bold; font-family:Verdana; font-size:16px; color:#FBFAFC; \">Punctuations</span>\n","\n","Similar to upper and lower case, punctuation marks do not contain a measurement value. For example, if we think that we are making a classification example, there are 2 reviews one of these has much more points/commas. In this situation, we do NOT expect any pattern from punctions. While this is the common approach, it may vary depending on the business problem."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:31.619433Z","iopub.status.busy":"2022-11-01T23:05:31.617025Z","iopub.status.idle":"2022-11-01T23:05:31.651027Z","shell.execute_reply":"2022-11-01T23:05:31.649762Z","shell.execute_reply.started":"2022-11-01T23:05:31.619398Z"},"trusted":true},"outputs":[],"source":["# check sentences and replace punctions with spaces\n","df['Review'] = df['Review'].str.replace('[^\\w\\s]', '')"]},{"cell_type":"markdown","metadata":{},"source":["This usage is called **\"Regular Expression\"**. Regular expression is a special way used to identify, capture and work with a particular pattern in textual expressions. The details of the regular expression can be searched, the general usage is given above. We will not be detailed here.\n","\n","***"]},{"cell_type":"markdown","metadata":{},"source":["<span class=\"label label-default\" style=\"background-color:#581845; border-radius:3px; font-weight: bold; font-family:Verdana; font-size:16px; color:#FBFAFC; \">Numbers</span>\n","\n","<div class=\"alert alert-danger\" role=\"alert\">\n","        <b> REMINDER: </b> <br>Like the above, our business problem may have discrimination about numbers. For instance, including much or fewer numbers can be crucial for the business problem. In this kind of problem, removing numbers is not a way we will use. In our example, we remove the numbers because the numbers are not distinctive.\n","</div>\n","<br>\n","\n","The general approach is to remove all problematic structures outside the text. For example, when there is social media data, we may choose to remove emojis, page links, etc. However, it may be such a task that we may need to focus on them.\n","\n","We capture numbers via regular expression. <code>\\d</code> help us to catch numbers."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:31.653404Z","iopub.status.busy":"2022-11-01T23:05:31.652751Z","iopub.status.idle":"2022-11-01T23:05:31.668725Z","shell.execute_reply":"2022-11-01T23:05:31.66751Z","shell.execute_reply.started":"2022-11-01T23:05:31.653368Z"},"trusted":true},"outputs":[],"source":["df['Review'] = df['Review'].str.replace('\\d', '')  "]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{},"source":["<span class=\"label label-default\" style=\"background-color:#581845; border-radius:3px; font-weight: bold; font-family:Verdana; font-size:16px; color:#FBFAFC; \">Stopwords</span>\n","\n","Commonly used words in the language do not have the measurement. Expressions such as various conjunctions such as *is*, *for*, *this*, or various pronouns have no metrical meaning. Therefore, we will delete these expressions.\n","\n","We have to download \"stopwords\" from nltk library. It has been prepared for English, there are studies for other languages as well. Therefore, you can use the language you want as an argument."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:31.671252Z","iopub.status.busy":"2022-11-01T23:05:31.670503Z","iopub.status.idle":"2022-11-01T23:05:31.822851Z","shell.execute_reply":"2022-11-01T23:05:31.821557Z","shell.execute_reply.started":"2022-11-01T23:05:31.671207Z"},"trusted":true},"outputs":[],"source":["nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:31.82552Z","iopub.status.busy":"2022-11-01T23:05:31.82476Z","iopub.status.idle":"2022-11-01T23:05:31.834329Z","shell.execute_reply":"2022-11-01T23:05:31.832818Z","shell.execute_reply.started":"2022-11-01T23:05:31.825475Z"},"trusted":true},"outputs":[],"source":["#¬†save the downloaded stopwords\n","sw = stopwords.words('english')"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-block alert-info\">\n","        <b> üìå We need to go to the reviews and delete if there are stopwords in the text or select outside words of these stopwords.</b>\n","</div>\n","<br>\n","\n","The process:\n","* <code>apply</code> allows function navigation, then let's write an apply function with <code>lambda</code>.\n","* Let's go through the reviews, <code>split</code> each line to get all the words one by one.\n","* After splitting, let's look at each word one by one (for example, with the help of the list comprehension), then, take the words that are not in stopwords, and <code>join</code> these words."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:31.838578Z","iopub.status.busy":"2022-11-01T23:05:31.837916Z","iopub.status.idle":"2022-11-01T23:05:32.140064Z","shell.execute_reply":"2022-11-01T23:05:32.139032Z","shell.execute_reply.started":"2022-11-01T23:05:31.838543Z"},"trusted":true},"outputs":[],"source":["df['Review'] = df['Review'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in sw))"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-success alert-info\">\n","       <b> üìå There may be words that we want to remove, we can write them as a list and remove the words we do not want with the code above. </b>\n","</div>\n","<br>\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["<span class=\"label label-default\" style=\"background-color:#581845; border-radius:3px; font-weight: bold; font-family:Verdana; font-size:16px; color:#FBFAFC; \">Rarewords</span>\n","\n","This section may not always be in the NLP process. Keeping rare words in modeling processes is a situation we generally do not want. We may want to remove them under our control. In other words, we may want to exclude words that make no sense, similar to logic of <span style=\"color:#FF0000\">Stopwords</span> section."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:32.142017Z","iopub.status.busy":"2022-11-01T23:05:32.141706Z","iopub.status.idle":"2022-11-01T23:05:32.177677Z","shell.execute_reply":"2022-11-01T23:05:32.17674Z","shell.execute_reply.started":"2022-11-01T23:05:32.141989Z"},"trusted":true},"outputs":[],"source":["#¬†to find how many times words counts\n","temp_df = pd.Series(' '.join(df['Review']).split()).value_counts()\n","temp_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:32.207989Z","iopub.status.busy":"2022-11-01T23:05:32.207593Z","iopub.status.idle":"2022-11-01T23:05:32.213922Z","shell.execute_reply":"2022-11-01T23:05:32.212813Z","shell.execute_reply.started":"2022-11-01T23:05:32.207957Z"},"trusted":true},"outputs":[],"source":["# choose the words with less than 2 frequencies to drop\n","drops = temp_df[temp_df <= 1]"]},{"cell_type":"markdown","metadata":{},"source":["The process is similar to Stopwords section. Look the \"Review\" column, split the reviews, then, join the words that are not in **drops**."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:32.480181Z","iopub.status.busy":"2022-11-01T23:05:32.479549Z","iopub.status.idle":"2022-11-01T23:05:32.569348Z","shell.execute_reply":"2022-11-01T23:05:32.56838Z","shell.execute_reply.started":"2022-11-01T23:05:32.480144Z"},"trusted":true},"outputs":[],"source":["df['Review'] = df['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in drops))"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-danger\" role=\"alert\">\n","        <b> NOTE: </b> <br> We could make a single string for preprocessing and go over it, but we're trying to stay at the dataframe level. Since working on dataframe is a more difficult scenario, we proceed here like this.\n","</div>\n","<br>\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["<span class=\"label label-default\" style=\"background-color:#581845; border-radius:3px; font-weight: bold; font-family:Verdana; font-size:16px; color:#FBFAFC; \">Lemmatization</span>\n","\n","It is the process of reducing the words into their roots. For example, removing the plural suffix \"s\" at the end of words.\n","\n","For this, we have to do the following downloads first."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:32.947069Z","iopub.status.busy":"2022-11-01T23:05:32.946349Z","iopub.status.idle":"2022-11-01T23:05:33.267161Z","shell.execute_reply":"2022-11-01T23:05:33.26573Z","shell.execute_reply.started":"2022-11-01T23:05:32.947021Z"},"trusted":true},"outputs":[],"source":["nltk.download('wordnet')\n","nltk.download('omw-1.4')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:33.27041Z","iopub.status.busy":"2022-11-01T23:05:33.269797Z","iopub.status.idle":"2022-11-01T23:05:35.607817Z","shell.execute_reply":"2022-11-01T23:05:35.606642Z","shell.execute_reply.started":"2022-11-01T23:05:33.270357Z"},"trusted":true},"outputs":[],"source":["df['Review'] = df['Review'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-success alert-info\">\n","       <b> üìå We got rid of many situations that would be problematic in the data set. In our own works, there may be different studies in terms of language or structuralism, we have eliminated the structures that should be removed from a text in general terms. </b>\n","</div>\n","<br>\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id = \"3\"></a>\n","# 3. Visualization üñº\n","\n","We will make some visualizations for exploratory data analysis. It is not for modeling, it is for observation and knowing data better.\n","\n","### <span style=\"color:#1F51FF\"><b>Calculation of Term Frequencies</b></span>\n","\n","We can use Bar Plot, WordCloud or visualization techniques that can be used for categorical features by obtaining a numeric value.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:35.610251Z","iopub.status.busy":"2022-11-01T23:05:35.609851Z","iopub.status.idle":"2022-11-01T23:05:39.933786Z","shell.execute_reply":"2022-11-01T23:05:39.932573Z","shell.execute_reply.started":"2022-11-01T23:05:35.610216Z"},"trusted":true},"outputs":[],"source":["# extract the term frequencies(frequency of the words) and create a df\n","tf = df[\"Review\"].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()\n","tf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:39.935564Z","iopub.status.busy":"2022-11-01T23:05:39.935235Z","iopub.status.idle":"2022-11-01T23:05:39.939939Z","shell.execute_reply":"2022-11-01T23:05:39.93913Z","shell.execute_reply.started":"2022-11-01T23:05:39.935535Z"},"trusted":true},"outputs":[],"source":["# fix the column names\n","tf.columns = [\"words\", \"tf\"]  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:39.943004Z","iopub.status.busy":"2022-11-01T23:05:39.942069Z","iopub.status.idle":"2022-11-01T23:05:39.961527Z","shell.execute_reply":"2022-11-01T23:05:39.960321Z","shell.execute_reply.started":"2022-11-01T23:05:39.94297Z"},"trusted":true},"outputs":[],"source":["# to see the most frequent words\n","tf.sort_values(\"tf\", ascending=False)"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-warning alert-info\">\n","        <b> üìå Above, we have a way to observe, not in terms of modeling, but in terms of exploring and analyzing.</b>\n","</div>\n","<br>\n","\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["<span class=\"label label-default\" style=\"background-color:#581845; border-radius:3px; font-weight: bold; font-family:Verdana; font-size:16px; color:#FBFAFC; \">Bar Plot</span>\n","\n","We have thousands of words. Since it would not be logical to show them all, let's take the ones with acceptable frequencies.\n","> **300 has been chosen here, but it is optional and can be changed.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:39.963682Z","iopub.status.busy":"2022-11-01T23:05:39.963333Z","iopub.status.idle":"2022-11-01T23:05:40.378251Z","shell.execute_reply":"2022-11-01T23:05:40.376891Z","shell.execute_reply.started":"2022-11-01T23:05:39.963651Z"},"trusted":true},"outputs":[],"source":["tf[tf[\"tf\"] > 300].plot.bar(x=\"words\", y=\"tf\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-success alert-info\">\n","       <b> üìå We came here without knowing which product these reviews belong to. The benefit of this kind of plotting is that after seeing these words, we can get an idea according to the words such as \"curtain\", \"room\", \"window\" etc. It seems like it is a textile product, and the reviews are about \"curtain\". </b>\n","</div>\n","<br>\n","\n","The examples of analysis we can make after this:\n","* We have the \"Star\" of the product. If the reviews that have less than 3 stars are filtered, we can the words for the problems of this product, and these problems can be fixed in manufacturing. \n","* Similarly, if the filter process is done for reviews that have more than 3 stars, the strong sides of the product can be revealed.\n","* If we did this for 5 different products, we could see on which subjects the products received the most likes or complaints. \n","---"]},{"cell_type":"markdown","metadata":{},"source":["<span class=\"label label-default\" style=\"background-color:#581845; border-radius:3px; font-weight: bold; font-family:Verdana; font-size:16px; color:#FBFAFC; \">Word Cloud</span>\n","\n","It allows to creation cloud shaped visuals according to the frequency of the words in the relevant text.\n","\n","<div class=\"alert alert-block alert-info\">\n","        <b> üìå For this, we need to specify all the texts as a single text. So, we have to make a single line to all elements of Review column.</b>\n","</div>\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:40.381289Z","iopub.status.busy":"2022-11-01T23:05:40.380827Z","iopub.status.idle":"2022-11-01T23:05:40.389372Z","shell.execute_reply":"2022-11-01T23:05:40.387967Z","shell.execute_reply.started":"2022-11-01T23:05:40.381245Z"},"trusted":true},"outputs":[],"source":["# check every row and join with these with a space\n","text = \" \".join(i for i in df.Review) "]},{"cell_type":"markdown","metadata":{},"source":["> **WordCloud performs the getting and counting of all the words that are in a text format. It performs the operations automatically that we have done manually above. It extracts the frequencies and reflects on the graph according to the frequencies.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:40.391877Z","iopub.status.busy":"2022-11-01T23:05:40.391088Z","iopub.status.idle":"2022-11-01T23:05:41.318637Z","shell.execute_reply":"2022-11-01T23:05:41.317858Z","shell.execute_reply.started":"2022-11-01T23:05:40.391832Z"},"trusted":true},"outputs":[],"source":["wordcloud = WordCloud().generate(text)\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:41.320292Z","iopub.status.busy":"2022-11-01T23:05:41.319833Z","iopub.status.idle":"2022-11-01T23:05:42.082364Z","shell.execute_reply":"2022-11-01T23:05:42.081246Z","shell.execute_reply.started":"2022-11-01T23:05:41.320262Z"},"trusted":true},"outputs":[],"source":["# configure the graph\n","wordcloud = WordCloud(max_font_size=50,\n","                      max_words=100,\n","                      background_color=\"white\").generate(text)\n","plt.figure()\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["<a id = \"4\"></a>\n","# 4. Sentiment Analysis üòçüòï\n","This section will be divided into 2 parts. The first part is to explain methods and how to use sentiment analysis. The second part is to implementation of sentiment analysis in our dataset. \n","\n","1. Sentiment Analysis Tutorial üìñ\n","2. Implementation to Our Dataset ‚úçÔ∏è\n","\n","<span class=\"label label-default\" style=\"background-color:#581845; border-radius:3px; font-weight: bold; font-family:Verdana; font-size:16px; color:#FBFAFC; \">Sentiment Analysis Tutorial üìñ</span>\n","\n","Sentiment Analysis aims to express the emotional state of the texts in a mathematical way. Let's say we have a sentence, each word in it has positive/negative/neutral meanings. These meanings are evaluated holistically and evaluations are made about whether a text is positive or negative."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:42.084132Z","iopub.status.busy":"2022-11-01T23:05:42.083758Z","iopub.status.idle":"2022-11-01T23:05:42.092136Z","shell.execute_reply":"2022-11-01T23:05:42.091069Z","shell.execute_reply.started":"2022-11-01T23:05:42.084102Z"},"trusted":true},"outputs":[],"source":["df[\"Review\"].head()"]},{"cell_type":"markdown","metadata":{},"source":["For instance, \"good\" has a negative meaning here. There is a pre-trained model in nltk for the meanings of words that can carry negative / positive values in this way. Such models are called <span style=\"color:#FF0000\"><b>pretty rate models</b></span>. Thanks to this pretty rate model, we find existing word sets and scores. These scores are called <span style=\"color:#FF0000\"><b>polarity scores</b></span>. \n","\n","For this, we have to do the following download first."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:42.096179Z","iopub.status.busy":"2022-11-01T23:05:42.095825Z","iopub.status.idle":"2022-11-01T23:05:42.113426Z","shell.execute_reply":"2022-11-01T23:05:42.112126Z","shell.execute_reply.started":"2022-11-01T23:05:42.096149Z"},"trusted":true},"outputs":[],"source":["nltk.download('vader_lexicon')  # pre-trained model for sentiment analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:42.114938Z","iopub.status.busy":"2022-11-01T23:05:42.114596Z","iopub.status.idle":"2022-11-01T23:05:42.133928Z","shell.execute_reply":"2022-11-01T23:05:42.132745Z","shell.execute_reply.started":"2022-11-01T23:05:42.114908Z"},"trusted":true},"outputs":[],"source":["#¬†Example 1\n","sia = SentimentIntensityAnalyzer()\n","sia.polarity_scores(\"The film was awesome\")"]},{"cell_type":"markdown","metadata":{},"source":["* **negative score = 0**\n","* **neutral score = 0.42**\n","* **positive score = 0.57**\n","* **compound score = 0.62**\n","\n","<div class=\"alert alert-block alert-info\">\n","        <b> üìå OUR FOCUS IS THE <ins>COMPOUND SCORE</ins>. Scores from the Compound Score are between -1/1. If the score is less than 0, the text is negative; if it is higher than 0, it means the text is positive.</b>\n","</div>\n","<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:42.137028Z","iopub.status.busy":"2022-11-01T23:05:42.136541Z","iopub.status.idle":"2022-11-01T23:05:42.143665Z","shell.execute_reply":"2022-11-01T23:05:42.142855Z","shell.execute_reply.started":"2022-11-01T23:05:42.136989Z"},"trusted":true},"outputs":[],"source":["#¬†Example 2\n","sia.polarity_scores(\"I liked this music but it is not good as the other one\")"]},{"cell_type":"markdown","metadata":{},"source":["---\n","<span class=\"label label-default\" style=\"background-color:#581845; border-radius:3px; font-weight: bold; font-family:Verdana; font-size:16px; color:#FBFAFC; \">Implementation to Our Dataset ‚úçÔ∏è</span>\n","\n","It means that we can implement this method on Review column and extract the polarity score of each text according to above information."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:42.145607Z","iopub.status.busy":"2022-11-01T23:05:42.145044Z","iopub.status.idle":"2022-11-01T23:05:42.161315Z","shell.execute_reply":"2022-11-01T23:05:42.160486Z","shell.execute_reply.started":"2022-11-01T23:05:42.145575Z"},"trusted":true},"outputs":[],"source":["df[\"Review\"][0:10].apply(lambda x: sia.polarity_scores(x)) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:42.163005Z","iopub.status.busy":"2022-11-01T23:05:42.162678Z","iopub.status.idle":"2022-11-01T23:05:42.178389Z","shell.execute_reply":"2022-11-01T23:05:42.177162Z","shell.execute_reply.started":"2022-11-01T23:05:42.162976Z"},"trusted":true},"outputs":[],"source":["# take only compound scores\n","df[\"Review\"][0:10].apply(lambda x: sia.polarity_scores(x)[\"compound\"]) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:42.180401Z","iopub.status.busy":"2022-11-01T23:05:42.180067Z","iopub.status.idle":"2022-11-01T23:05:43.351155Z","shell.execute_reply":"2022-11-01T23:05:43.35012Z","shell.execute_reply.started":"2022-11-01T23:05:42.180373Z"},"trusted":true},"outputs":[],"source":["#¬†save the scores as a new column\n","df[\"polarity_score\"] = df[\"Review\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-success alert-info\">\n","       <b> üìå Sentiment analysis is generally used to use users' satisfaction and is a method that attracts attention. In fact, sector-specific sentiment analyzes are also carried out on special word sets prepared specifically for the relevant sets. </b>\n","</div>\n","<br>\n","\n","> <span style=\"color:#FF0000\"><b>So what will the polarity_score we get do?</b></span><br>\n","<b>There are different types of work that can be done depending on the approach. For example, those who have scores less than 3 in the overall variable (the star users give to the product) and write positive sentences (polarity_score>0) can be deduced from this, and the contrasts and anomalies in the data set can be examined. Similarly, it is possible to check low stars with high polarity_score, there is a possibility that those people gave wrong scores. These people can be reached and the stars can be fixed.<br><br>\n","Sentiment analysis can be enriched according to different approaches. Product benchmarking, brand benchmarking, competitor analysis etc. can be done. It can be done on Amazon or other platforms as well.</b>"]},{"cell_type":"markdown","metadata":{},"source":["<a id = \"5\"></a>\n","# 5. Feature Engineering üèó\n","\n","**Firstly, we have to understand how to convert an unsupervised learning problem to a supervised learning problem.**\n","<center><img src=\"https://learn.g2crowd.com/hubfs/unsupervised-learning.png\" width=\"500\"/></center>\n","\n","<div class=\"alert alert-block alert-info\">\n","        <b> üìå Let's say we have an unsupervised learning method. To make this supervised, we create labels (like positive-negative). Then, we choose the labels we created as the target variable, so that when a new data comes in, we get which class it belongs to as a result of the model.<br><br>\n","            üèÅ We can look at the model building phase as starting here because we will create a label and select it as the target variable, and now we will have a supervised learning question.\n","</b>\n","</div>\n","<br>\n","\n","Let's start with an example, then, apply it to all variables on Review column."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:43.353996Z","iopub.status.busy":"2022-11-01T23:05:43.353594Z","iopub.status.idle":"2022-11-01T23:05:43.366716Z","shell.execute_reply":"2022-11-01T23:05:43.365414Z","shell.execute_reply.started":"2022-11-01T23:05:43.35394Z"},"trusted":true},"outputs":[],"source":["# we have such values and we will create a new variable by taking all of them so that we have a label.\n","df[\"Review\"][0:10].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:43.368738Z","iopub.status.busy":"2022-11-01T23:05:43.368258Z","iopub.status.idle":"2022-11-01T23:05:43.392744Z","shell.execute_reply":"2022-11-01T23:05:43.391681Z","shell.execute_reply.started":"2022-11-01T23:05:43.368592Z"},"trusted":true},"outputs":[],"source":["# if we want to see this example with new label and pos/neg side by side\n","rev_pol = pd.concat([df[\"Review\"][0:10], df[\"Review\"][0:10].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")], axis=1)\n","rev_pol.columns = [\"Review\", \"Polarity Scores\"]\n","rev_pol"]},{"cell_type":"markdown","metadata":{},"source":["> **Let's assign a new variable with a polarity score greater than 0 to be positive and the smaller ones to be negative.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:43.395384Z","iopub.status.busy":"2022-11-01T23:05:43.394876Z","iopub.status.idle":"2022-11-01T23:05:44.576202Z","shell.execute_reply":"2022-11-01T23:05:44.57501Z","shell.execute_reply.started":"2022-11-01T23:05:43.395343Z"},"trusted":true},"outputs":[],"source":["df[\"sentiment_label\"] = df[\"Review\"].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")\n","df[\"sentiment_label\"].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["> **Let's look at the averages of scores of positive/negative comments.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.577857Z","iopub.status.busy":"2022-11-01T23:05:44.577513Z","iopub.status.idle":"2022-11-01T23:05:44.588415Z","shell.execute_reply":"2022-11-01T23:05:44.587607Z","shell.execute_reply.started":"2022-11-01T23:05:44.577826Z"},"trusted":true},"outputs":[],"source":["df.groupby(\"sentiment_label\")[\"Star\"].mean()"]},{"cell_type":"markdown","metadata":{},"source":["<span style=\"color:#008000\"><b>It seems logical and we are on the right road! ü•≥</b></span>\n","\n","> **Our label has a string naming as pos/neg which is not binary encoded, let's pass it through label_encoder.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.590192Z","iopub.status.busy":"2022-11-01T23:05:44.589831Z","iopub.status.idle":"2022-11-01T23:05:44.598731Z","shell.execute_reply":"2022-11-01T23:05:44.597942Z","shell.execute_reply.started":"2022-11-01T23:05:44.590161Z"},"trusted":true},"outputs":[],"source":["df[\"sentiment_label\"] = LabelEncoder().fit_transform(df[\"sentiment_label\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.600159Z","iopub.status.busy":"2022-11-01T23:05:44.599727Z","iopub.status.idle":"2022-11-01T23:05:44.610783Z","shell.execute_reply":"2022-11-01T23:05:44.60967Z","shell.execute_reply.started":"2022-11-01T23:05:44.600108Z"},"trusted":true},"outputs":[],"source":["y = df[\"sentiment_label\"]\n","X = df[\"Review\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.612443Z","iopub.status.busy":"2022-11-01T23:05:44.612117Z","iopub.status.idle":"2022-11-01T23:05:44.630999Z","shell.execute_reply":"2022-11-01T23:05:44.629886Z","shell.execute_reply.started":"2022-11-01T23:05:44.612413Z"},"trusted":true},"outputs":[],"source":["print(\" X \".center(50, \"~\"))\n","display(X.head())\n","print(\"\")\n","print(\" Y \".center(50, \"~\"))\n","display(y.head())"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-danger alert-info\">\n","        <b> ü§î As you can see, we got arguments in X, but what we have is text and not variable/variables, no measurement values, we need to bring them to measurable format. We need to generate features from these texts, we need to derive features that are measurable and can be put into mathematical operations.<br><br>\n","            üö® THE MOST CRITICAL POINT OF NLP WORKS IS THE NUMERICAL REPRESENTATIONS OF TEXTS AND WORDS. IN OTHER EXPRESSIONS, THEY ARE THE STUDY OF VECTORING WORDS.\n","</b>\n","</div>\n","<br>\n","\n","\n","\n","\n","\n","In other words, we need to perform such operations on X so that we can bring it into a measurable format, on which mathematical operations and machine learning modeling can be performed. For this we need to create word vectors. Commonly used methods:\n","- Count Vectors \n","- TF-IDF\n","- Another Word Embeddings Methods(Word2Vec, GloVe, BERT etc.)\n","\n","We will analyze the implementation of Count Vectors and TF-IDF. Another word embedding methods can be researched and applied with similar preprocess(such as remove punctions, numbers etc.). Each of these methods is the method used by the computer to put these texts into mathematical operations in the world of linear algebra. The text I have is in the form of a text and I have to do something with this text so that it can be processed in the linear algebra world. Let's start to explanation of methods and application on our dataset."]},{"cell_type":"markdown","metadata":{},"source":["<span class=\"label label-default\" style=\"background-color:#581845; border-radius:3px; font-weight: bold; font-family:Verdana; font-size:16px; color:#FBFAFC; \">Count Vectors üíØ</span>\n","\n","<span class=\"label label-default\" \n","     style=\"color:white;\n","           display:fill;\n","           border-radius:5px;\n","           background-color:crimson;\n","           font-size:100%;\n","           font-family:Verdana;\n","           letter-spacing:0.5px\">\n","Explanation and Examples\n","</span>\n","\n","Count vectors means subtracting the frequencies of words. For example, it is to extract how many times the words in the review are used. So how do we represent these words (words, characters, ngrams).\n","\n","Method  | Description |\n","--- | --- \n","Word | Numerical representations of each word (Ex: numerical, word, each)\n","Characters | Numerical representations of each characters(Ex: n, u, m, e, r etc.)\n","Ngam | Refers to producing features according to word phrases\n","\n","Let's make an example for <span style=\"color:#FF0000\"><b>ngrams</b></span> for better understaing."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.632888Z","iopub.status.busy":"2022-11-01T23:05:44.632456Z","iopub.status.idle":"2022-11-01T23:05:44.653395Z","shell.execute_reply":"2022-11-01T23:05:44.652319Z","shell.execute_reply.started":"2022-11-01T23:05:44.632853Z"},"trusted":true},"outputs":[],"source":["a = \"\"\"ngram is a contiguous sequence of n items from a given sample of text or speech.\"\"\"\n","\n","TextBlob(a).ngrams(3)  # For example, let's create a triple ngram"]},{"cell_type":"markdown","metadata":{},"source":["> **Groups a word in 3, moves on to the next word and groups it in 3, etc. So ngram is to break words into chunks with groups.**\n","\n","So, we can count <span style=\"color:#FF0000\"><b>words</b></span>, <span style=\"color:#008000\"><b>characters</b></span>, <span style=\"color:#0000FF\"><b>ngrams</b></span>.\n","\n","For example, we have 4 sentences (1st document, 2nd document etc.), it can be anything we get here, for example 1st tweet, 2nd tweet etc. If we look at our example dataset, we have 1st review, 2nd review... In short, it refers to different units that we are interested in."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.6552Z","iopub.status.busy":"2022-11-01T23:05:44.654865Z","iopub.status.idle":"2022-11-01T23:05:44.660072Z","shell.execute_reply":"2022-11-01T23:05:44.659135Z","shell.execute_reply.started":"2022-11-01T23:05:44.65517Z"},"trusted":true},"outputs":[],"source":["corpus = ['This is the first document.',\n","          'This document is the second document.',\n","          'And this is the third one.',\n","          'Is this the first document?']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.661869Z","iopub.status.busy":"2022-11-01T23:05:44.661442Z","iopub.status.idle":"2022-11-01T23:05:44.678746Z","shell.execute_reply":"2022-11-01T23:05:44.677581Z","shell.execute_reply.started":"2022-11-01T23:05:44.661827Z"},"trusted":true},"outputs":[],"source":["# word frequency\n","vectorizer = CountVectorizer()\n","X_c = vectorizer.fit_transform(corpus)  # we transformed the corpus with fit_transform\n","vectorizer.get_feature_names()  "]},{"cell_type":"markdown","metadata":{},"source":["> **We extract unique words (ie we thought the whole text as a single text and brought the unique words) => ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third' ', 'this']**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.681185Z","iopub.status.busy":"2022-11-01T23:05:44.680679Z","iopub.status.idle":"2022-11-01T23:05:44.687947Z","shell.execute_reply":"2022-11-01T23:05:44.686798Z","shell.execute_reply.started":"2022-11-01T23:05:44.68115Z"},"trusted":true},"outputs":[],"source":["X_c.toarray()  # we vectorized each unit"]},{"cell_type":"markdown","metadata":{},"source":["It means:\n","* First document includes 0 \"and\", 1 \"document\", 1 \"first\" etc.\n","* Second document includes 0 \"and\", 2 \"document\", 0 \"first\" etc.\n","* Third document includes 1 \"and\", 0 \"document\", 0 \"first\" etc.\n","* Fourth document includes 0 \"and\", 1 \"document\", 1 \"first\" etc.\n","---"]},{"cell_type":"markdown","metadata":{},"source":["For ngram, we will use an argument as **analyzer=\"word\"**, if we do not enter any argument, the default of CountVectorizer is to make *word frequency*."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.689595Z","iopub.status.busy":"2022-11-01T23:05:44.689297Z","iopub.status.idle":"2022-11-01T23:05:44.70217Z","shell.execute_reply":"2022-11-01T23:05:44.701227Z","shell.execute_reply.started":"2022-11-01T23:05:44.689568Z"},"trusted":true},"outputs":[],"source":["# n-gram\n","vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n","X_n = vectorizer2.fit_transform(corpus)\n","vectorizer2.get_feature_names()  # it brought the words one by one above, now he brought them in phrases"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.710572Z","iopub.status.busy":"2022-11-01T23:05:44.70948Z","iopub.status.idle":"2022-11-01T23:05:44.718034Z","shell.execute_reply":"2022-11-01T23:05:44.716833Z","shell.execute_reply.started":"2022-11-01T23:05:44.710532Z"},"trusted":true},"outputs":[],"source":["X_n.toarray()"]},{"cell_type":"markdown","metadata":{},"source":["It means:\n","* First document includes 0 \"and this\", 0 \"document is\", 1 \"first document\" etc.\n","* Second document includes 0 \"and this\", 1 \"document is\", 0 \"first document\" etc.\n","* Third document includes 1 \"and this\", 0 \"document is\", 0 \"first document\" etc.\n","* Fourth document includes 0 \"and this\", 0 \"document is\", 1 \"first document\" etc.\n","---\n","\n","<span class=\"label label-default\" \n","     style=\"color:white;\n","           display:fill;\n","           border-radius:5px;\n","           background-color:crimson;\n","           font-size:100%;\n","           font-family:Verdana;\n","           letter-spacing:0.5px\">\n","Application on our dataset\n","\n","\n","Firstly, we will apply word count, then, ngram process will be done. "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.720335Z","iopub.status.busy":"2022-11-01T23:05:44.719951Z","iopub.status.idle":"2022-11-01T23:05:44.801916Z","shell.execute_reply":"2022-11-01T23:05:44.800741Z","shell.execute_reply.started":"2022-11-01T23:05:44.720302Z"},"trusted":true},"outputs":[],"source":["vectorizer = CountVectorizer()  # default => word count\n","X_count = vectorizer.fit_transform(X)  # X stands for texts, we only count the frequencies of the words"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.803919Z","iopub.status.busy":"2022-11-01T23:05:44.803553Z","iopub.status.idle":"2022-11-01T23:05:44.812133Z","shell.execute_reply":"2022-11-01T23:05:44.811039Z","shell.execute_reply.started":"2022-11-01T23:05:44.803887Z"},"trusted":true},"outputs":[],"source":["vectorizer.get_feature_names()[10:15]  # let's look at some features"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.814042Z","iopub.status.busy":"2022-11-01T23:05:44.813598Z","iopub.status.idle":"2022-11-01T23:05:44.84321Z","shell.execute_reply":"2022-11-01T23:05:44.842193Z","shell.execute_reply.started":"2022-11-01T23:05:44.814008Z"},"trusted":true},"outputs":[],"source":["X_count.toarray()[10:15]"]},{"cell_type":"markdown","metadata":{},"source":["> **Let's keep this aside, let's produce features according to other methods. For example, let's look at the TF-IDF method, derive features according to both word frequencies and ngrams, then compare the frequencies of these 3 methods in machine learning.**"]},{"cell_type":"markdown","metadata":{},"source":["<span class=\"label label-default\" style=\"background-color:#581845; border-radius:3px; font-weight: bold; font-family:Verdana; font-size:16px; color:#FBFAFC; \">TF-IDF (Term Frequency-Inverse Document Frequency) üì∂</span>\n","\n","<span class=\"label label-default\" \n","     style=\"color:white;\n","           display:fill;\n","           border-radius:5px;\n","           background-color:crimson;\n","           font-size:100%;\n","           font-family:Verdana;\n","           letter-spacing:0.5px\">\n","Explanation and Examples\n","\n","\n","**It is a normalized, standardized word vector generation method to eliminate some of the biases that the count vector method may reveal. Count Vectorizer will create biases in favor of words with high frequency and against other words. In order to eliminate this and standardize it, the TF-IDF method has been proposed.**\n","\n","\n","<div class=\"alert alert-block alert-info\">\n","        <b> ‚öôÔ∏è HOW IT WORKS: </b> A standardization process is performed on the focus of the frequency of the words in the documents and the frequency of the words in the whole corpus.\n","</div>\n","<br>\n"," \n","\n","<div class=\"alert alert-warning alert-info\">\n","        <b> ‚ö†Ô∏è REMINDER: </b>The most critical point of NLP studies is the effort to represent words/texts numerically, the work of creating a word vector. The method for this is Count Vector, TF-IDF or another Word Embedding methods(such as Word2Vec, GloVe, BERT which can be considered as more advanced methods).\n","</div>\n","<br>\n","\n","TF-IDF Steps:\n","1. Calculates Count Vectors\n","2. Calculate TF (frequency of t term in related document/total number of terms in document)\n","> In other words, we subtract the weight of a particular word in a particular document.\n","3. Calculate IDF( 1+loge( (total number of documents+1)/(number of documents with t term in it+1) )\n","> In terms of all documents, we extract information about the relevant terms.\n","\n","##### **In step 2, we focused on the words within each document, and in step 3, we try to consider the impact of terms on all documents.**\n","4. Calculate TF*IDF (we multiply the TF matrix by the vector IDF)\n","5. Perform L2 Normalization. Find the square root of the sum of the squares of the rows, divide all the cells in the corresponding row by the value you found.\n","\n","> Of course, the library will implement these steps for us and we will use the results coming from TF-IDF directly.\n","\n","When creating the Count Vector, there was a question whether we are doing this for words, characters, or ngrams. Similarly for TF-IDF, there is the question whether to do it for words, characters, or ngrams. As an example, let's do word and ngram as before.\n","\n","We will use \"corpus\" that we created above, again."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.845039Z","iopub.status.busy":"2022-11-01T23:05:44.844664Z","iopub.status.idle":"2022-11-01T23:05:44.859414Z","shell.execute_reply":"2022-11-01T23:05:44.858295Z","shell.execute_reply.started":"2022-11-01T23:05:44.845004Z"},"trusted":true},"outputs":[],"source":["# word frequency\n","tf_idf_word_vectorizer = TfidfVectorizer()  #¬†default = word frequency\n","corpus_tf_idf_word = tf_idf_word_vectorizer.fit_transform(corpus)\n","tf_idf_word_vectorizer.get_feature_names_out()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.861292Z","iopub.status.busy":"2022-11-01T23:05:44.860897Z","iopub.status.idle":"2022-11-01T23:05:44.876202Z","shell.execute_reply":"2022-11-01T23:05:44.875213Z","shell.execute_reply.started":"2022-11-01T23:05:44.861243Z"},"trusted":true},"outputs":[],"source":["corpus_tf_idf_word.toarray()"]},{"cell_type":"markdown","metadata":{},"source":["> **Same with Count Vectors, each text on corpus were taken and each words' TF-IDF scores were calculated above according to word frequency.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:44.877926Z","iopub.status.busy":"2022-11-01T23:05:44.877534Z","iopub.status.idle":"2022-11-01T23:05:45.347606Z","shell.execute_reply":"2022-11-01T23:05:45.346527Z","shell.execute_reply.started":"2022-11-01T23:05:44.877895Z"},"trusted":true},"outputs":[],"source":["# ngram\n","tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2, 3))\n","corpus_tf_idf_ngram = tf_idf_ngram_vectorizer.fit_transform(X)\n","tf_idf_ngram_vectorizer.get_feature_names_out()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:45.34935Z","iopub.status.busy":"2022-11-01T23:05:45.349008Z","iopub.status.idle":"2022-11-01T23:05:45.671843Z","shell.execute_reply":"2022-11-01T23:05:45.670687Z","shell.execute_reply.started":"2022-11-01T23:05:45.349321Z"},"trusted":true},"outputs":[],"source":["corpus_tf_idf_ngram.toarray()"]},{"cell_type":"markdown","metadata":{},"source":["> **Above, we can see the TF-IDF scores to ngrams.**"]},{"cell_type":"markdown","metadata":{},"source":["<span class=\"label label-default\" \n","     style=\"color:white;\n","           display:fill;\n","           border-radius:5px;\n","           background-color:crimson;\n","           font-size:100%;\n","           font-family:Verdana;\n","           letter-spacing:0.5px\">\n","Application on Our Dataset\n","\n","\n","Since our reviews are saved on X, we will use same process using X rather than corpus."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:45.673379Z","iopub.status.busy":"2022-11-01T23:05:45.673043Z","iopub.status.idle":"2022-11-01T23:05:45.75849Z","shell.execute_reply":"2022-11-01T23:05:45.75725Z","shell.execute_reply.started":"2022-11-01T23:05:45.673351Z"},"trusted":true},"outputs":[],"source":["# word\n","tf_idf_word_vectorizer = TfidfVectorizer()\n","X_tf_idf_word = tf_idf_word_vectorizer.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:45.760268Z","iopub.status.busy":"2022-11-01T23:05:45.759919Z","iopub.status.idle":"2022-11-01T23:05:46.156957Z","shell.execute_reply":"2022-11-01T23:05:46.155862Z","shell.execute_reply.started":"2022-11-01T23:05:45.760238Z"},"trusted":true},"outputs":[],"source":["# ngram\n","tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2, 3))\n","X_tf_idf_ngram = tf_idf_ngram_vectorizer.fit_transform(X)"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-block alert-info\">\n","        <b> üìå Finally, we created all of the features. As a reminde, our features are word frequency according to CountVectorizer and TF-IDF scores for words and ngrams. It means, we create 3 features.</b>\n","</div>\n","<br>\n","\n","---\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id = \"6\"></a>\n","# 6. Sentiment Modeling üíª\n","\n","Just remember our processes:\n","1. Import and Check Data\n","2. Text Preprocessing\n","3. Text Visualization\n","4. Sentiment Analysis\n","5. Feature Engineering\n","6. Sentiment Modeling\n","\n","Since we will treat it as a classification problem, we look at classification algorithms. In here, we will use Logisctic Regression and Random Forest. Another classification algorithms can be used, as well.\n","\n","Also, neural network can be used for Sentiment Analysis. Since the running time will be too long using LSTM, we will not show this method in this notebook. You can check my another notebook about this:\n","\n","[Sentiment Analysis with LSTM](https://www.kaggle.com/code/furkannakdagg/sentiment-analysis-with-lstm)\n","\n","<span class=\"label label-default\" style=\"background-color:#581845; border-radius:3px; font-weight: bold; font-family:Verdana; font-size:16px; color:#FBFAFC; \">Logistic Regression</span>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:46.158946Z","iopub.status.busy":"2022-11-01T23:05:46.158556Z","iopub.status.idle":"2022-11-01T23:05:46.534496Z","shell.execute_reply":"2022-11-01T23:05:46.533413Z","shell.execute_reply.started":"2022-11-01T23:05:46.158914Z"},"trusted":true},"outputs":[],"source":["# Words with TF-IDF\n","log_model = LogisticRegression().fit(X_tf_idf_word, y)\n","cross_val_score(log_model,\n","                X_tf_idf_word,\n","                y,\n","                scoring=\"accuracy\",\n","                cv=5).mean()"]},{"cell_type":"markdown","metadata":{},"source":["> **Let's predict if a new review comes out.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:51.542474Z","iopub.status.busy":"2022-11-01T23:05:51.54208Z","iopub.status.idle":"2022-11-01T23:05:51.547647Z","shell.execute_reply":"2022-11-01T23:05:51.546719Z","shell.execute_reply.started":"2022-11-01T23:05:51.542441Z"},"trusted":true},"outputs":[],"source":["# get a new comment\n","new_review = pd.Series(\"this product is great\")"]},{"cell_type":"markdown","metadata":{},"source":["> **We vectorized the words in the process, we can also vectorize the words of the new review and ask the model to predict.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:53.066018Z","iopub.status.busy":"2022-11-01T23:05:53.06559Z","iopub.status.idle":"2022-11-01T23:05:53.156948Z","shell.execute_reply":"2022-11-01T23:05:53.155571Z","shell.execute_reply.started":"2022-11-01T23:05:53.065978Z"},"trusted":true},"outputs":[],"source":["# vectorize the new review\n","new_review = TfidfVectorizer().fit(X).transform(new_review)\n","\n","# predict the sentiment of review\n","log_model.predict(new_review)"]},{"cell_type":"markdown","metadata":{},"source":["> **1 means positive review, 0 means negative reviews.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:53.390078Z","iopub.status.busy":"2022-11-01T23:05:53.389614Z","iopub.status.idle":"2022-11-01T23:05:53.479356Z","shell.execute_reply":"2022-11-01T23:05:53.478194Z","shell.execute_reply.started":"2022-11-01T23:05:53.390038Z"},"trusted":true},"outputs":[],"source":["# Let's pick a sample comment from the original dataset and ask it to the model\n","sample = df[\"Review\"].sample(1).values\n","print(sample)\n","random_review = pd.Series(sample)\n","new_review = TfidfVectorizer().fit(X).transform(random_review)\n","log_model.predict(new_review)"]},{"cell_type":"markdown","metadata":{},"source":["<span class=\"label label-default\" style=\"background-color:#581845; border-radius:3px; font-weight: bold; font-family:Verdana; font-size:16px; color:#FBFAFC; \">Random Forest</span>\n","\n","Let's create a model according to 3 different methods and compare their success."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:05:53.693494Z","iopub.status.busy":"2022-11-01T23:05:53.692796Z","iopub.status.idle":"2022-11-01T23:08:48.387319Z","shell.execute_reply":"2022-11-01T23:08:48.385909Z","shell.execute_reply.started":"2022-11-01T23:05:53.693455Z"},"trusted":true},"outputs":[],"source":["# Count Vectors\n","rf_model = RandomForestClassifier().fit(X_count, y)\n","print(\"Count Vectors Score\", cross_val_score(rf_model, X_count, y, cv=5, n_jobs=-1).mean()) \n","\n","# TF-IDF Word-Level\n","rf_model = RandomForestClassifier().fit(X_tf_idf_word, y)\n","print(\"TF-IDF Word-Level Score\", cross_val_score(rf_model, X_tf_idf_word, y, cv=5, n_jobs=-1).mean())\n","\n","# TF-IDF N-Gram\n","rf_model = RandomForestClassifier().fit(X_tf_idf_ngram, y)\n","print(\"TF-IDF N-Gram Score\", cross_val_score(rf_model, X_tf_idf_ngram, y, cv=5, n_jobs=-1).mean())"]},{"cell_type":"markdown","metadata":{},"source":["> **Let's plot these scores.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:08:48.391071Z","iopub.status.busy":"2022-11-01T23:08:48.390285Z","iopub.status.idle":"2022-11-01T23:11:41.881171Z","shell.execute_reply":"2022-11-01T23:11:41.880017Z","shell.execute_reply.started":"2022-11-01T23:08:48.391019Z"},"trusted":true},"outputs":[],"source":["%%time\n","def score(method_list):\n","    scores = []\n","    for method in method_list:\n","        rf_model = RandomForestClassifier().fit(method, y)\n","        scores.append(cross_val_score(rf_model, method, y, cv=5, n_jobs=-1).mean())\n","        \n","    scores_df = pd.DataFrame({\"Methods\": [\"Count Vectors\", \"TF-IDF Word-Level\", \"TF-IDF N-Gram\"],\n","                             \"Scores\": scores})\n","    \n","    trace1 = go.Bar(\n","                    y = scores_df[\"Methods\"],\n","                    x = scores_df[\"Scores\"],\n","                    name = \"Accuracy Plot\",\n","                    text=[round(i,5) for i in scores_df['Scores']],\n","                    marker = dict(color = ['#2ECC71','#34495E','#D0D3D4'],\n","                                 line=dict(color='rgb(0,0,0)',width=1.5)),\n","        orientation='h', textposition = 'inside'\n","    )\n","    data = [trace1]\n","    layout = go.Layout(barmode = \"group\", \n","                       title={'text': \"Scores\" ,\n","                                 'y':0.9,\n","                                 'x':0.5,\n","                                 'xanchor': 'center',\n","                                 'yanchor': 'top'},\n","                          template='plotly_white')\n","    fig = go.Figure(data = data, layout = layout)\n","    pio.show(fig)\n","    \n","method_list = [X_count, X_tf_idf_word, X_tf_idf_ngram]\n","score(method_list)"]},{"cell_type":"markdown","metadata":{},"source":["<div class=\"alert alert-block alert-info\">\n","        <b> üìå The 3 methods above were the feature generation method. We built models for each and looked at 5-fold cross validation.</b>\n","</div>\n","<br>\n","\n","<span class=\"label label-default\" \n","     style=\"color:white;\n","           display:fill;\n","           border-radius:5px;\n","           background-color:crimson;\n","           font-size:120%;\n","           font-family:Verdana;\n","           letter-spacing:0.5px\">\n","Hyperparameter Optimization\n","</span>\n","\n","Since we got the highest success from the Count Vectors model, let's continue with it. Firstly, let's explain the method that we will use called as **GridSearchCV**.\n","\n","<span style=\"color:#FF0000\"><b>GridSearchCV:</b></span> A separate model is created with all combinations for the hyperparameters and their values that are desired to be tested in the model, and the most successful hyperparameter set is determined according to the specified metric. Simply, we will give a parameter set, and the method will try the all of the values we give on the set, and get the best result."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:11:41.883304Z","iopub.status.busy":"2022-11-01T23:11:41.882873Z","iopub.status.idle":"2022-11-01T23:11:41.889629Z","shell.execute_reply":"2022-11-01T23:11:41.888604Z","shell.execute_reply.started":"2022-11-01T23:11:41.883261Z"},"trusted":true},"outputs":[],"source":["rf_model = RandomForestClassifier(random_state=17)  # create an empty model object"]},{"cell_type":"markdown","metadata":{},"source":["We apply GridSearchCV with a small number of parameter values in order not to prolong the process. More parameters and more values can be experimented with, this will increase the processing time so we give a brief overview here."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:11:41.892438Z","iopub.status.busy":"2022-11-01T23:11:41.891821Z","iopub.status.idle":"2022-11-01T23:12:49.650375Z","shell.execute_reply":"2022-11-01T23:12:49.649211Z","shell.execute_reply.started":"2022-11-01T23:11:41.892406Z"},"trusted":true},"outputs":[],"source":["%%time\n","rf_params = {\"max_depth\": [8, None],\n","             \"max_features\": [7, \"auto\"],\n","             \"min_samples_split\": [2, 5, 8],\n","             \"n_estimators\": [100, 200]\n","             }\n","\n","rf_best_grid = GridSearchCV(rf_model,\n","                            rf_params,\n","                            cv=5,\n","                            n_jobs=-1,\n","                            verbose=1).fit(X_count, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:12:49.652223Z","iopub.status.busy":"2022-11-01T23:12:49.651771Z","iopub.status.idle":"2022-11-01T23:12:49.661368Z","shell.execute_reply":"2022-11-01T23:12:49.659599Z","shell.execute_reply.started":"2022-11-01T23:12:49.65218Z"},"trusted":true},"outputs":[],"source":["# best parameters that GridSearchCV found\n","rf_best_grid.best_params_"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-11-01T23:12:49.663453Z","iopub.status.busy":"2022-11-01T23:12:49.663004Z","iopub.status.idle":"2022-11-01T23:12:56.333716Z","shell.execute_reply":"2022-11-01T23:12:56.332589Z","shell.execute_reply.started":"2022-11-01T23:12:49.663411Z"},"trusted":true},"outputs":[],"source":["#¬†set the model with the parameters GridSearchCV found above\n","rf_final = rf_model.set_params(**rf_best_grid.best_params_, random_state=17).fit(X_count, y)\n","\n","# our final score\n","cross_val_score(rf_final, X_count, y, cv=5, n_jobs=-1).mean()"]},{"cell_type":"markdown","metadata":{},"source":["<center style=\"font-family:cursive; font-size:18px; color:#159364;\">Please vote and comment if you enjoyed my kernel, thanks for reading! ü•≥</center>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":2588484,"sourceId":4418575,"sourceType":"datasetVersion"}],"dockerImageVersionId":30301,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
